---
title: "夢語りボット"
color: "secondary"
updated: "2022-07-29T12:00Z"
featuredImage: "./cascade.jpg"
tags: 
    - "Tag Decoder"
---

**このページの内容は推敲中です**

チャットボットの反応を変化に富んだものにするために、人間がさまざまな回答を事前に考えて辞書を作るのは非常に大変な作業です。
前のページでは差し替えによって反応のバリエーションを与える方法を検討しましたが、素材を集める部分での工夫はしたものの結局内容をチャットボット製作者が考える必要がありました。
ここで<Link to="/article/inside/2-basic/04-text-generation/01-markov">マルコフ連鎖による文生成</Link>を用いればログやコーパスを自動的に辞書化することができます。
マルコフ連鎖による文生成では、素材となるテキストデータの状況によりますが、マルコフ連鎖をランダムにたどっていく性質上生成されるテキストの意味はあまり予想できません。
つまりユーザの特定のセリフに対応した適切な反応に使うよりも、「何でもあり」な文脈にマルコフ連鎖を組み込むほうが考えやすくなります。
「何でもあり」な文脈とは、例えば

* 最近見た夢の内容、三題噺
* 特定できない「誰か」のツイート＋チャットボットの感想
* 架空のニュース(フェイクニュース化しないよう注意)、噂話、都市伝説
* 失敗談
* リスナーからのお手紙

などです。
夢、誰かのツイート、架空ニュース、失敗談は一日の会話の始まりで雑談のきっかけとしてトリガされても面白いでしょう。
またユーザが失敗談を話したあとで「私もこんな失敗があったよ」という態で同じ種類の話題を切り出すと返報性の法則にも従うので自然になりそうです。

リスナーからのお手紙はチャットボットやユーザがラジオに出演しているというような場面設定が必要な例ですが、定期的に挟むと面白そうです。

## テキストの収集方法

### twitter

### 青空文庫
[青空文庫](https://www.aozora.gr.jp/)では著作権保護期間の終了した作品16,000件以上、著作権存続中の作品300件以上を収録しており、著作権保護期間の終了した作品は自由に改変が可能なので、チャットボットの辞書の素材として利用できます。
日本の作家では例えば宮沢賢治、夏目漱石、芥川竜之介、太宰修、中島敦、梶井基次郎、森鴎外、夢野旧作、海外の作家ではフランツ・カフカ、ジョナサン・スウィフト、モーリス・ルブラン、ゲーテ、エドガー・アラン・ポオなどがそれにあたります。
日本の作家名を見ると想像できるように、作品はいずれも現代の言葉遣いとは異なるためちょっと特殊なチャットボットになりそうですが、海外の作品では違和感なく使えるものもたくさんあります。

### Project Gutenberg
[Project Gutenberg](https://www.gutenberg.org/)
では英語を始めとした様々な言語の書籍69,000件以上をダウンロードできます。[日本語の文献](https://www.gutenberg.org/browse/languages/ja)も20件ほどあります。日本語でないテキストは翻訳サイトで日本語化すればチャットボットの素材として利用できるでしょう。

### 自作

## 



マルコフ連鎖を用いる利点の一つは辞書を人間が読んでそれなりに理解可能であることです。
人間が辞書を読むことで不適切な発言の可能性を事前に発見できm
→元は不適切でなくてもつながり的に不適切化する
→単語の時点でアウト


チャットボットの反応をバリエーションに富んだものにすることは重要ですが、人間がさまざまな回答を事前に考えて辞書を作るのは非常に大変な作業です。

マルコフ連鎖では、直前の一文字を見て次の一文字を決める方法だとログが大きくなるにつれ生成されるテキストが無秩序化してしまう傾向がありますが、連鎖の単位を文字ではなく形態素にしたり、先行する文字を一文字から複数文字にするといった方法でそれを軽減することができる、ということを見てきました。
Fig. 1は<Link to="/article/inside/2-basic/04-text-generation/01-markov">マルコフ連鎖による文生成</Link>で示したものであり、横軸が次候補の数で縦がその頻度を表すヒストグラムで、青がuni-gram(先行する一文字を考慮)、赤がbi-gram(先行する二文字を考慮)です。
これを見ると次候補の数が1である左端のピーク高さはbi-gramで0.7となっており、元の文書を再現する確率が高いことが分かります。また次候補の数が2以上の裾野はbi-gramの方が小さくなっていることからも同様の傾向が見て取れます。
つまりこの方法では無秩序化の軽減が進むと同時に元のテキストの単なる再現に近くなってしまうということで、これではあまり面白くありません。
自然言語としての秩序を保ったままテキスト生成の幅を広げることはできるでしょうか。

![Fig. 1. 二文字を考慮したマルコフ連鎖のヒストグラム](../04-text-generation/2gram.png)

テキストが不自然かどうかを計算することは難しく、例えば鈴木ら[^1]はn-gramに対応したニューラルネットワークを作り、それによる判断を試みています。
しかしマルコフ連鎖で生成したテキストはそもそもn-gramのルールに則っているため、今回の場合は検出力がないと思われます。

[^1]:[鈴井 克徳ら, DEIM Forum 2018 G3-4](https://db-event.jpn.org/deim2018/data/papers/298.pdf)

そこで、上述のページで使ったデータについてテキストが不自然になる状況を具体的に調べてみます。文字のuni-gramだとマルコフ連鎖の中身を見ても意味がわからないので、形態素の1-gramマルコフ連鎖を用います。
マルコフ連鎖うちの次候補の数の多いものを見ると、「の」「を」「が」などがあります。

![Fig. 2. 形態素によるマルコフ連鎖の次候補数](../04-text-generation/morpho-1gram-top.png)

「の」の次候補はこのようになっています。

```
    "の": [
        "侵入",
        "万",
        "融解",
        "意",
        "だ",
        "発する",
        "だ",
        "写真",
        "だ",
        "で"
    ],
```

これらを見ると、「〜の写真」のように一つの概念を構成する単語をつなぐ「の」の他に、強調表現の「のだ」、理由を説明する「ので」のように意味的には分割しないほうが良さそうなものが混在しています。
まずは「のだ」「ので」などは一つのノードとして再構成すると不自然さを軽減する効果があるでしょう。
なお、「のだ」や「ので」における「の」は準体助詞、「〜の写真」における「の」は格助詞に分類されるため、表層語彙だけでなく品詞情報も考慮したマルコフ連鎖の生成も有効です。

次に「を」に続く形態素をみてみましょう。

```
    "を": [
        "知ら",
        "聞き入れ",
        "余儀",
        "伴う",
        "述べる",
        "開示",
        "除い",
        "見合わせ"
    ],
```

こちらは動詞や動詞として機能する名詞（サ変接続名詞）が集まっています。ここで、動詞やサ変接続名詞が集まっているのなら、人間が他の動詞を辞書に付け加えることでチャットボットの反応を拡張できそうです。
ゼロからチャットボットの反応を考えるのに比べて「これ以外の動詞を付け加えて」というお題であれば考えるのは用意になります。
動詞よりももっと膨らませやすそうなのは名詞です。
名詞が現れるパターンを前述のテキストから探すと、

* これまで発表を見合わせてきた写真
* 通常の写真
* 航空写真

これらは文法的には非常に違った姿をしていますが、全て意味的に交換可能です。共通しているのは末尾が「写真」であることで、これは一つの概念を記述するとき、前の語句が係り、末尾の語が受けになるというルールからきています。
つまり、日本語の場合は後ろから前へのマルコフ連鎖を組み立てるとこれらのパターンは自動的に組み込まれるようになる





考えると、「りんごが好き」「ライオンが好き」「プリウスが好き」のように、名詞の後に決まったbi-gramが続くものが挙げられます。
さらに「〇〇が好き」パターンに当てはまった〇〇は単に名詞の集合というだけでなく「好みのもの」という一つの概念を形成すると言っても良いかもしれません。
概念化されれば人間は新しい候補をさらに考えやすくなりますし、逆に不適切な候補を除外することも容易になります。

1. 名詞+接尾辞でノードを再構成
1. 助詞+動詞でノードを再構成
2. 準体助詞関連でノードを再構成
3. 前からマルコフ連鎖を生成
4. 辞書の中で共通する「が好き」を見つけたら「は→Y→が好き」というパターンを「は→(Y→が好き)」に書き換えてYの候補を

* ログをそのまま使う→ログ型

* マルコフ連鎖で何をきっかけに任意のマルコフ連鎖がスタートするのかを決めるのは難しいが、「夢の話」「お告げ」「占い」などの体であればおもしろい
* 小説を使う場合は地の文と発言「」を分離してそれぞれ別のマルコフ連鎖にする→スタックのあるマルコフ連鎖






## タグを展開するエンコーダ
例えば辞書に以下のような記述をします。

```
in:[
    "{animal}"
],
out:[
    "犬","猫","イルカ","アゲハチョウ","恐竜"
],
in:[
    "好きな動物は？"
],
out:[
    "{animal}が好きだよ"
]
```

ここでinやoutのなかで`{animal}`と書かれている部分をタグと呼びます。outの文字列にタグが含まれたらinの中に同じタグを見つけて、その中からランダムに一つを使います。
これにより、「`{animal}`が好きだよ」という辞書の文字列は「犬が好きだよ」「猫が好きだよ」・・・としてユーザへの返答になります。

この処理をするためには

## 辞書の可読性と概念化

ここで、辞書の可読性について考えます。

* 助詞と動詞＋送り仮名、サ変動詞の結合、形態素→句読点の結合
* 文末から文頭に向かうマルコフ連鎖生成。文頭文字を付ける必要があり、
* セリフと地の文の区別



りんごが好き
猫が好き
景色を見るのが好き
猫を見るのが好き

のようにかかり受けが右から左になる場合が多いので、マルコフ連鎖も末尾から先頭に向かって進める
これにより「好きな何か」という概念がマルコフ連鎖の中に形成され、それを人間が後から改造しやすくなる

辞書化する際は

{node001}:["りんご","猫","{node003}を見るの"]
{node002}:["{node001}が好き"]
{node003}:["景色","猫"]

のようになるはず。
