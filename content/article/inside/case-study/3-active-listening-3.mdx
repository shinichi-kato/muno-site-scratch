---
title: "傾聴ボット(3)"
color: "secondary"
updated: "2022-07-12T12:00Z"
featuredImage: "../abstract2.png"
tags: 
    - "test"
---

## 要約のためのencoder設計

前の章では傾聴の中でも基礎的な技法を選び、シンプルな構成のチャットボットで試してみました。


### 7. 短い要約を返す

1〜6を使ったチャットボットでは、例えばユーザ（話し手）の入力文字列が下記inのどれかに似ていたら、outの文字列を返答にするという辞書で作ることができました。

> in: "えーっと、", "うーん", "なんと言ったらいいか"
> out: "ゆっくり考えてもらって大丈夫ですよ。"

ところが要約の場合はユーザのセリフの一部を記憶してそれを埋め込んだ返答をすることがポイントです。

> ユーザ: 今日は隣の街のモールに行って、本屋でマンガを買ってきたよ
> チャットボット:マンガを？

つまり、inが単純な文字列ではなく **○○を買ってきた** のようなパターンになります。そしてユーザの入力文字列がこのパターンと一致したとき、「○」に当たる部分の言葉を記憶して以降の出力文字列でそれを利用します。
ここで、よく似た方法が取り入れられたElizaの辞書を見てみましょう。
Elizaの辞書では、会話機能の中核部分はキー`key`、分解`decomp`と再構成`reasmb`からなっています。

```yaml
key: remember 5
  decomp: * i remember *
    reasmb: Do you often think of (2) ?
    reasmb: Does thinking of (2) bring anything else to mind ?
    reasmb: What else do you recollect ?
    reasmb: Why do you remember (2) just now ?
    reasmb: What in the present situation reminds you of (2) ?
    reasmb: What is the connection between me and (2) ?
    reasmb: What else does (2) remind you of ?
```

この辞書では、ユーザの入力に`key`の`"remeber"`が含まれていた場合、`decomp`のパターンに従って `"i remember"`より前の文を(1)に、後の文を(2)に代入します。
続いて`reasmb`のうちどれか一つを選んで(1)や(2)を当てはめて返答とします。
例えばユーザが「**At the moment** I remember **his face**」と言ってきたら、(1)=**At the moment**、(2)=**his face**という抽出を行い、「Do you ofthen think of **his face**」という返答を生成します。
この仕掛けは一見すると日本語にも移植できそうに見えますが、日本語では語順に融通が効き、主語をはじめとして両者で共通認識している語は省略することが好まれます。
またtfidfのような統計的な類似度を用いた検索ではなく明確なパターンマッチングによってエンコードを行っているため、対応力はそれほど大きくありません。

そこで、**語順の入れ替わりに対応しつつキーワードを抽出できるエンコーダー**を考えます。

#### bag-of-文節エンコーダー

日本語は語順に融通が効きます。例えば文を文節に区切ってみると

> 米田さんは|図書館に|傘を持って|でかけた
> 図書館に|傘を持って|米田さんは|でかけた

のように、「米田さんは」「図書館に」「傘を持って」の３つは入れ替えても文の意味がほとんど変わりません。
一方最後の動詞「でかけた」は位置を変えるとニュアンスが変わり、先頭に持ってくると意味はなんとなくわかりますが、不自然になります。

> 図書館に|でかけた|米田さんは|傘を持って
> でかけた|傘を持って|米田さんは|図書館に

次に同じ文を[UniDic-MeCab](http://www4414uj.sakura.ne.jp/Yasanichi1/unicheck/)で形態素に区切ります。

> 米田 | さん | は | 図書館 | に | 傘 | を | 持っ | て | でかけ | た

形態素単位で語順を入れ替えてしまうと、意味は壊れてしまうようです。

> は | 図書館 | さん | 米田 | を | 持っ | でかけ | た | て | に | 傘

つまり、語順に融通がきくというのをより細かく言うと、

**文節の順序**は変えてもあまり意味が変化しない

といえるでしょう。それぞれの文節の最後は助詞つまり「てにをは」で終わっています。

| 書字形 | 品詞	|
| :--    | :-- |
| 米田 | 名詞-固有名詞-人名-姓 |
| さん | 接尾辞-名詞的-一般 |
| は   | **助詞**-係助詞 |
| 図書 | 名詞-普通名詞-一般 |
| 館   | 接尾辞-名詞的-一般	|
| に   | **助詞**-格助詞 |
| 傘   | 名詞-普通名詞-一般 |
| を   | **助詞**-格助詞 |
| 持っ | 動詞-一般 |
| て   | **助詞**-接続助詞 |
| でかけ | 動詞-一般 |
| た     | 助動詞 |

それを目印にすることで形態素ではなく文節に分けることができるでしょう。

#### bag-of-phrase(文節)

自然言語処理の中では、一つの文に含まれる単語を単語ごとに数えたものをテキストの内部表現にする方法があり、単語の数は見ても単語の順番は無視していることからbag-of-wordsと呼びます。
ここでbag-of-wordsが語順を無視することと文節の順序は融通がきくということは相性が良さそうです。
つまりbag-of-phrase(文節)を考えれば日本語におけるテキストの分析が自然になるのではないでしょうか。
様々な形態素解析エンジンやKNPなどをもちいれば形態素解析と文節への区切りが可能になります。一方で形態素解析には巨大な辞書が必要で、webブラウザ上で簡単に運用できるものではありません。
一方形態素解析のような品詞の情報がない分かち書きであればブラウザ上でも動作する[TinySegmenter](http://chasen.org/~taku/software/TinySegmenter/)が利用できます。
前者を使えば精度は高いが速度は低く、計算資源は多くなります。後者では逆に精度は低いですが、軽量で高速です。

てbag-of-wordならぬ**bag-of-文節(phrase)**を
word vectorを使ったエンコードを考えます。
word vectorは[分かち書き](http://chasen.org/~taku/software/TinySegmenter/)によって文字列を単語などに分割し、各要素ごとに数を数えたものをベクトル化したものです。

・bag-of-word
・分かち書き
・分節区切り化


(以下製作中)
