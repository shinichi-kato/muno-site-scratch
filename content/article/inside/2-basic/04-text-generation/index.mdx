---
title: 自動テキスト生成
color: "secondary"
updated: "2022-11-03T12:00Z"
featuredImage: sculpture.jpg
tags: 
    - "テキスト生成"
    - "自然言語解析"
---

チャットボットの会話では、チャットボットが同じような発言ばかりしたり、関係のない返事をしてしまうことが課題になっています。
誰かからこのような発言されたとき、聞き手の人間は話し手が強い不満を感じているのではないかと感じたり、話し手からバカにされたような気持ちにさせられてしまうためです。  
チャットボットを**エンコーダー**、**内的プロセス**、**デコーダー**に分けて考えたとき、関係のない返事をしてしまうのは**エンコーダー**でユーザの入力を適切に認識できていない、という課題とみなすことができます。
一方同じ発言の繰り返しは**内的プロセス**であればこれまでの会話を考慮できていない、**デコーダー**であれば返答生成用の辞書のレパートリーが少なすぎる、ということが課題だと言えます。

今回はこれらのうち**デコーダー**に着目し、その解決策を探ります。

返答生成用の辞書の返答を豊かなものにするには、巨大でかつ質の高い辞書が求められます。手動では質の高い辞書を作ることができるものの、いわゆる想定問答集のようなものをひたすら考えるのはなかなか大変な作業です。 
機械学習やマルコフ連鎖などの統計的な手法を用いることで、元になるデータさえあればサイズの大きな辞書を作ることができます。

## 自動文生成の手法

### マルコフ連鎖によるテキスト生成
マルコフ連鎖とはテキストを文字や形態素といったノードに分割し、「次にくるノードはなにか」を確率過程として表現したものです。
直前の一単語から次にくる単語を予測するものを一階のマルコフ連鎖、直前の２単語から次を予測するものを二階のマルコフ連鎖、のように呼びます。
チャットやSNSのログ、小説などを元にしてマルコフ連鎖辞書を生成すると、元のテキストで使われる単語を使って文らしきものを生成することができます。
web上でマルコフ連鎖によるテキスト生成を試せるサイト[^1]やマルコフ連鎖のためのpythonモジュール[^2]も利用できます。

[^1]: [PHP Markov chain text generator](https://projects.haykranen.nl/markov/demo/)
[^2]: [markovify](https://github.com/jsvine/markovify)

### RNNによるテキスト生成

機械学習で自然言語を扱う例として「ニュース記事の分類」を考えてみます。
ニュース記事に現れる単語に注目すると天気、経済、スポーツなどに特異的なものやどれにでも現れるものなど様々です。
機械学習ではこの記事を入力とし、「天気」のようなラベルを出力として学習し、未知のニュースを与えたときにそれがどの分類になるかを推測させます。 
テキスト生成ではこれ応用し、各単語を入力、その次に現れる単語を出力とする、例えば「台風」=入力、「12号」=出力というデータを学習させます。
これにより単語を次々に予測することでテキストが生成できます。 

ここで、直前の単語だけを見ているとテキストが不自然になりやすいため、ニューラルネットワークが「ニューラルネットワークの直前の状態」も加味して次の出力を予測する、という工夫が考えられました。
これを**リカレントニューラルネットワーク(RNN)**と呼びます[^3]。

[^3]: [RNN によるテキスト生成](https://www.tensorflow.org/tutorials/text/text_generation?hl=ja)

### LSTMによるテキスト生成

機械学習を利用したもう一つの有名な方法にLSTMがあります[^4]。LSTM(Long Short-Term Memory)はRNNよりも長期的な情報を考慮したテキスト生成が可能な方法です。
RNNでは一つ前のニューラルネットワークの状態を利用しましたが、LSTMでは影響を残すべきでない情報の除去（忘却）、新たに取り入れるべき情報の選別、出力すべき情報の選別などの機能が追加されています。
今回のテーマではありませんが、RNNやLSTMは入力をユーザのセリフ、出力をチャットボットの返答とすることで**エンコーダー** + **内的プロセス** + **デコーダー**を構成することができます。
なお、これらRNNやLSTMを用いる方法では学習にも発話にも巨大なストレージと計算資源が必要になります。そのためチャットボットの運用には何らかのサーバーが必要になります。

[^4]: [LSTMネットワークの概要](https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca)


## チャットボットで用いる手法

マルコフ連鎖、RNN、LSTMのいずれを用いた場合でも、チャットボット用の辞書をSNSのログなどから生成する際には不特定多数の個人が様々な状況で発言したデータの寄せ集めになり、チャットボットとして一つの個性を表現するのは困難になっていきます。
またテキストにはチャットボット製作者が考慮しきれないような不適切発言が混入しており、トラブルが起きる前にそれを除去することが一つの大きな課題になっています。
2016年にMicrosoftが公開したTwitterチャットボットTayはユーザ発言を学習するようになっていました。
これに目をつけたユーザがいたずらで人種差別的な発言をチャットボットに教えてしまい、それが問題となってTayは公開後24h未満で閉鎖になってしまいました。[^5]

[^5]: [Twitter taught Microsoft's AI chatbot to be a racist asshole in less than a day](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)

このように大量で雑多なデータやリアルタイムに増えるデータを利用すると、チャットボット製作者のチェックが追いつかなくなることは想定しておくべきでしょう。
これに加えて機械学習の場合は学習モデルを観察しても、なにをトリガとして不適切な発言をするのかを読み取ることが困難で、不適切な発言だけをピンポイントで学習モデルから除去するということもあまり現実的ではないと思います。

一方、マルコフ連鎖の辞書は機械学習のモデルと比べてコンパクトでそれなりの可読性があり、発話に必要な計算資源は非常に少ないというメリットがあります。
そこで、以降の章ではマルコフ連鎖を使ったテキスト生成について詳しく議論していきます。